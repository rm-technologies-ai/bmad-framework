## GitLab Epics - atlas-datascience/lion

## Epic #10: Establish AWS multi account configuration for dev, test, ops
- **State**: opened
- **Start**: N/A
- **Due**: N/A
- **Created**: 2025-07-10T15:11:03.407Z
- **Web**: https://gitlab.com/groups/atlas-datascience/lion/-/epics/10



N/A = if (@{id=3687571; iid=10; color=#1068bf; text_color=#FFFFFF; group_id=101972113; parent_id=; parent_iid=; imported=False; imported_from=none; title=Establish AWS multi account configuration for dev, test, ops; description=; confidential=False; author=; start_date=; start_date_is_fixed=False; start_date_fixed=; start_date_from_inherited_source=; start_date_from_milestones=; end_date=; due_date=; due_date_is_fixed=False; due_date_fixed=; due_date_from_inherited_source=; due_date_from_milestones=; state=opened; web_edit_url=/groups/atlas-datascience/lion/-/epics/10; web_url=https://gitlab.com/groups/atlas-datascience/lion/-/epics/10; references=; created_at=2025-07-10T15:11:03.407Z; updated_at=2025-07-10T15:30:25.867Z; closed_at=; labels=System.Object[]; upvotes=0; downvotes=0; _links=}.start_date) { @{id=3687571; iid=10; color=#1068bf; text_color=#FFFFFF; group_id=101972113; parent_id=; parent_iid=; imported=False; imported_from=none; title=Establish AWS multi account configuration for dev, test, ops; description=; confidential=False; author=; start_date=; start_date_is_fixed=False; start_date_fixed=; start_date_from_inherited_source=; start_date_from_milestones=; end_date=; due_date=; due_date_is_fixed=False; due_date_fixed=; due_date_from_inherited_source=; due_date_from_milestones=; state=opened; web_edit_url=/groups/atlas-datascience/lion/-/epics/10; web_url=https://gitlab.com/groups/atlas-datascience/lion/-/epics/10; references=; created_at=2025-07-10T15:11:03.407Z; updated_at=2025-07-10T15:30:25.867Z; closed_at=; labels=System.Object[]; upvotes=0; downvotes=0; _links=}.start_date } else { 'N/A' }
N/A = if (@{id=3687571; iid=10; color=#1068bf; text_color=#FFFFFF; group_id=101972113; parent_id=; parent_iid=; imported=False; imported_from=none; title=Establish AWS multi account configuration for dev, test, ops; description=; confidential=False; author=; start_date=; start_date_is_fixed=False; start_date_fixed=; start_date_from_inherited_source=; start_date_from_milestones=; end_date=; due_date=; due_date_is_fixed=False; due_date_fixed=; due_date_from_inherited_source=; due_date_from_milestones=; state=opened; web_edit_url=/groups/atlas-datascience/lion/-/epics/10; web_url=https://gitlab.com/groups/atlas-datascience/lion/-/epics/10; references=; created_at=2025-07-10T15:11:03.407Z; updated_at=2025-07-10T15:30:25.867Z; closed_at=; labels=System.Object[]; upvotes=0; downvotes=0; _links=}.due_date) { @{id=3687571; iid=10; color=#1068bf; text_color=#FFFFFF; group_id=101972113; parent_id=; parent_iid=; imported=False; imported_from=none; title=Establish AWS multi account configuration for dev, test, ops; description=; confidential=False; author=; start_date=; start_date_is_fixed=False; start_date_fixed=; start_date_from_inherited_source=; start_date_from_milestones=; end_date=; due_date=; due_date_is_fixed=False; due_date_fixed=; due_date_from_inherited_source=; due_date_from_milestones=; state=opened; web_edit_url=/groups/atlas-datascience/lion/-/epics/10; web_url=https://gitlab.com/groups/atlas-datascience/lion/-/epics/10; references=; created_at=2025-07-10T15:11:03.407Z; updated_at=2025-07-10T15:30:25.867Z; closed_at=; labels=System.Object[]; upvotes=0; downvotes=0; _links=}.due_date } else { 'N/A' }

--- 
## Epic #9: Ingestion Gateway
- **State**: opened
- **Start**: N/A
- **Due**: N/A
- **Created**: 2025-07-08T14:28:55.272Z
- **Web**: https://gitlab.com/groups/atlas-datascience/lion/-/epics/9

[Ingestion-Gateway](https://gitlab.com/groups/ftsg/lion/-/wikis/Project-Lion-Platform/Architecture/Components/Ingestion-Gateway)

N/A = if (@{id=3684515; iid=9; color=#1068bf; text_color=#FFFFFF; group_id=101972113; parent_id=; parent_iid=; imported=False; imported_from=none; title=Ingestion Gateway; description=[Ingestion-Gateway](https://gitlab.com/groups/ftsg/lion/-/wikis/Project-Lion-Platform/Architecture/Components/Ingestion-Gateway); confidential=False; author=; start_date=; start_date_is_fixed=False; start_date_fixed=; start_date_from_inherited_source=; start_date_from_milestones=; end_date=; due_date=; due_date_is_fixed=False; due_date_fixed=; due_date_from_inherited_source=; due_date_from_milestones=; state=opened; web_edit_url=/groups/atlas-datascience/lion/-/epics/9; web_url=https://gitlab.com/groups/atlas-datascience/lion/-/epics/9; references=; created_at=2025-07-08T14:28:55.272Z; updated_at=2025-07-10T19:22:06.692Z; closed_at=; labels=System.Object[]; upvotes=0; downvotes=0; _links=}.start_date) { @{id=3684515; iid=9; color=#1068bf; text_color=#FFFFFF; group_id=101972113; parent_id=; parent_iid=; imported=False; imported_from=none; title=Ingestion Gateway; description=[Ingestion-Gateway](https://gitlab.com/groups/ftsg/lion/-/wikis/Project-Lion-Platform/Architecture/Components/Ingestion-Gateway); confidential=False; author=; start_date=; start_date_is_fixed=False; start_date_fixed=; start_date_from_inherited_source=; start_date_from_milestones=; end_date=; due_date=; due_date_is_fixed=False; due_date_fixed=; due_date_from_inherited_source=; due_date_from_milestones=; state=opened; web_edit_url=/groups/atlas-datascience/lion/-/epics/9; web_url=https://gitlab.com/groups/atlas-datascience/lion/-/epics/9; references=; created_at=2025-07-08T14:28:55.272Z; updated_at=2025-07-10T19:22:06.692Z; closed_at=; labels=System.Object[]; upvotes=0; downvotes=0; _links=}.start_date } else { 'N/A' }
N/A = if (@{id=3684515; iid=9; color=#1068bf; text_color=#FFFFFF; group_id=101972113; parent_id=; parent_iid=; imported=False; imported_from=none; title=Ingestion Gateway; description=[Ingestion-Gateway](https://gitlab.com/groups/ftsg/lion/-/wikis/Project-Lion-Platform/Architecture/Components/Ingestion-Gateway); confidential=False; author=; start_date=; start_date_is_fixed=False; start_date_fixed=; start_date_from_inherited_source=; start_date_from_milestones=; end_date=; due_date=; due_date_is_fixed=False; due_date_fixed=; due_date_from_inherited_source=; due_date_from_milestones=; state=opened; web_edit_url=/groups/atlas-datascience/lion/-/epics/9; web_url=https://gitlab.com/groups/atlas-datascience/lion/-/epics/9; references=; created_at=2025-07-08T14:28:55.272Z; updated_at=2025-07-10T19:22:06.692Z; closed_at=; labels=System.Object[]; upvotes=0; downvotes=0; _links=}.due_date) { @{id=3684515; iid=9; color=#1068bf; text_color=#FFFFFF; group_id=101972113; parent_id=; parent_iid=; imported=False; imported_from=none; title=Ingestion Gateway; description=[Ingestion-Gateway](https://gitlab.com/groups/ftsg/lion/-/wikis/Project-Lion-Platform/Architecture/Components/Ingestion-Gateway); confidential=False; author=; start_date=; start_date_is_fixed=False; start_date_fixed=; start_date_from_inherited_source=; start_date_from_milestones=; end_date=; due_date=; due_date_is_fixed=False; due_date_fixed=; due_date_from_inherited_source=; due_date_from_milestones=; state=opened; web_edit_url=/groups/atlas-datascience/lion/-/epics/9; web_url=https://gitlab.com/groups/atlas-datascience/lion/-/epics/9; references=; created_at=2025-07-08T14:28:55.272Z; updated_at=2025-07-10T19:22:06.692Z; closed_at=; labels=System.Object[]; upvotes=0; downvotes=0; _links=}.due_date } else { 'N/A' }

--- 
## Epic #8: Cross-Account STS Role Framework for the Access Broker
- **State**: opened
- **Start**: N/A
- **Due**: N/A
- **Created**: 2025-06-30T15:59:04.840Z
- **Web**: https://gitlab.com/groups/atlas-datascience/lion/-/epics/8


To honor the platform's "data-in-place" promise and its Least-Privilege & Short-Lived Credentials principle, we need a reusable, Terraform-first framework that lets the Access Broker assume tightly-scoped roles in any customer or staging account, using AWS STS with an ExternalId guard. The epic will deliver IaC module plus reference policies, CI checks, and integration tests so that the Access Broker can fetch a pre-signed S3 URL or RDS IAM token across account. It must support multiple trust patterns (single-tenant prod, sandbox, and shared-services accounts), emit CloudTrail and Athena-queryable audit records for every AssumeRole.


**Acceptance Criteria**

- Terraform module published in infra-iac/modules/sts-cross-account with variables for tenant_id, allowed_actions, external_id, max_session_seconds.
- AssumeRole trust policy enforces sts:ExternalId and restricts the principal to our Access Broker execution role ARN list.
- Managed-policy generator outputs the minimal IAM JSON based on a YAML allow-list of data-stores the tenant actually uses (e.g., s3:GetObject for specific buckets, rds-db:connect on a named cluster).
- CI pipeline runs tflint, terrascan, and checkov and fails if any policy grants broader than requested, or if max_session_seconds exceeds 900.
- End-to-end integration test spins up a throw-away account in AWS Control Tower, deploys the module, then exercises the Broker's /v1/issue endpoint to retrieve a presigned S3 URL and verifies object download succeeds while an out-of-scope bucket is denied.
- CloudTrail + Athena dashboards show every AssumeRole with sourceIdentity=access-broker and surface decision_id for audit.

N/A = if (@{id=3674247; iid=8; color=#1068bf; text_color=#FFFFFF; group_id=101972113; parent_id=; parent_iid=; imported=False; imported_from=none; title=Cross-Account STS Role Framework for the Access Broker; description=
To honor the platform's "data-in-place" promise and its Least-Privilege & Short-Lived Credentials principle, we need a reusable, Terraform-first framework that lets the Access Broker assume tightly-scoped roles in any customer or staging account, using AWS STS with an ExternalId guard. The epic will deliver IaC module plus reference policies, CI checks, and integration tests so that the Access Broker can fetch a pre-signed S3 URL or RDS IAM token across account. It must support multiple trust patterns (single-tenant prod, sandbox, and shared-services accounts), emit CloudTrail and Athena-queryable audit records for every AssumeRole.


**Acceptance Criteria**

- Terraform module published in infra-iac/modules/sts-cross-account with variables for tenant_id, allowed_actions, external_id, max_session_seconds.
- AssumeRole trust policy enforces sts:ExternalId and restricts the principal to our Access Broker execution role ARN list.
- Managed-policy generator outputs the minimal IAM JSON based on a YAML allow-list of data-stores the tenant actually uses (e.g., s3:GetObject for specific buckets, rds-db:connect on a named cluster).
- CI pipeline runs tflint, terrascan, and checkov and fails if any policy grants broader than requested, or if max_session_seconds exceeds 900.
- End-to-end integration test spins up a throw-away account in AWS Control Tower, deploys the module, then exercises the Broker's /v1/issue endpoint to retrieve a presigned S3 URL and verifies object download succeeds while an out-of-scope bucket is denied.
- CloudTrail + Athena dashboards show every AssumeRole with sourceIdentity=access-broker and surface decision_id for audit.; confidential=False; author=; start_date=; start_date_is_fixed=False; start_date_fixed=; start_date_from_inherited_source=; start_date_from_milestones=; end_date=; due_date=; due_date_is_fixed=False; due_date_fixed=; due_date_from_inherited_source=; due_date_from_milestones=; state=opened; web_edit_url=/groups/atlas-datascience/lion/-/epics/8; web_url=https://gitlab.com/groups/atlas-datascience/lion/-/epics/8; references=; created_at=2025-06-30T15:59:04.840Z; updated_at=2025-06-30T15:59:04.840Z; closed_at=; labels=System.Object[]; upvotes=0; downvotes=0; _links=}.start_date) { @{id=3674247; iid=8; color=#1068bf; text_color=#FFFFFF; group_id=101972113; parent_id=; parent_iid=; imported=False; imported_from=none; title=Cross-Account STS Role Framework for the Access Broker; description=
To honor the platform's "data-in-place" promise and its Least-Privilege & Short-Lived Credentials principle, we need a reusable, Terraform-first framework that lets the Access Broker assume tightly-scoped roles in any customer or staging account, using AWS STS with an ExternalId guard. The epic will deliver IaC module plus reference policies, CI checks, and integration tests so that the Access Broker can fetch a pre-signed S3 URL or RDS IAM token across account. It must support multiple trust patterns (single-tenant prod, sandbox, and shared-services accounts), emit CloudTrail and Athena-queryable audit records for every AssumeRole.


**Acceptance Criteria**

- Terraform module published in infra-iac/modules/sts-cross-account with variables for tenant_id, allowed_actions, external_id, max_session_seconds.
- AssumeRole trust policy enforces sts:ExternalId and restricts the principal to our Access Broker execution role ARN list.
- Managed-policy generator outputs the minimal IAM JSON based on a YAML allow-list of data-stores the tenant actually uses (e.g., s3:GetObject for specific buckets, rds-db:connect on a named cluster).
- CI pipeline runs tflint, terrascan, and checkov and fails if any policy grants broader than requested, or if max_session_seconds exceeds 900.
- End-to-end integration test spins up a throw-away account in AWS Control Tower, deploys the module, then exercises the Broker's /v1/issue endpoint to retrieve a presigned S3 URL and verifies object download succeeds while an out-of-scope bucket is denied.
- CloudTrail + Athena dashboards show every AssumeRole with sourceIdentity=access-broker and surface decision_id for audit.; confidential=False; author=; start_date=; start_date_is_fixed=False; start_date_fixed=; start_date_from_inherited_source=; start_date_from_milestones=; end_date=; due_date=; due_date_is_fixed=False; due_date_fixed=; due_date_from_inherited_source=; due_date_from_milestones=; state=opened; web_edit_url=/groups/atlas-datascience/lion/-/epics/8; web_url=https://gitlab.com/groups/atlas-datascience/lion/-/epics/8; references=; created_at=2025-06-30T15:59:04.840Z; updated_at=2025-06-30T15:59:04.840Z; closed_at=; labels=System.Object[]; upvotes=0; downvotes=0; _links=}.start_date } else { 'N/A' }
N/A = if (@{id=3674247; iid=8; color=#1068bf; text_color=#FFFFFF; group_id=101972113; parent_id=; parent_iid=; imported=False; imported_from=none; title=Cross-Account STS Role Framework for the Access Broker; description=
To honor the platform's "data-in-place" promise and its Least-Privilege & Short-Lived Credentials principle, we need a reusable, Terraform-first framework that lets the Access Broker assume tightly-scoped roles in any customer or staging account, using AWS STS with an ExternalId guard. The epic will deliver IaC module plus reference policies, CI checks, and integration tests so that the Access Broker can fetch a pre-signed S3 URL or RDS IAM token across account. It must support multiple trust patterns (single-tenant prod, sandbox, and shared-services accounts), emit CloudTrail and Athena-queryable audit records for every AssumeRole.


**Acceptance Criteria**

- Terraform module published in infra-iac/modules/sts-cross-account with variables for tenant_id, allowed_actions, external_id, max_session_seconds.
- AssumeRole trust policy enforces sts:ExternalId and restricts the principal to our Access Broker execution role ARN list.
- Managed-policy generator outputs the minimal IAM JSON based on a YAML allow-list of data-stores the tenant actually uses (e.g., s3:GetObject for specific buckets, rds-db:connect on a named cluster).
- CI pipeline runs tflint, terrascan, and checkov and fails if any policy grants broader than requested, or if max_session_seconds exceeds 900.
- End-to-end integration test spins up a throw-away account in AWS Control Tower, deploys the module, then exercises the Broker's /v1/issue endpoint to retrieve a presigned S3 URL and verifies object download succeeds while an out-of-scope bucket is denied.
- CloudTrail + Athena dashboards show every AssumeRole with sourceIdentity=access-broker and surface decision_id for audit.; confidential=False; author=; start_date=; start_date_is_fixed=False; start_date_fixed=; start_date_from_inherited_source=; start_date_from_milestones=; end_date=; due_date=; due_date_is_fixed=False; due_date_fixed=; due_date_from_inherited_source=; due_date_from_milestones=; state=opened; web_edit_url=/groups/atlas-datascience/lion/-/epics/8; web_url=https://gitlab.com/groups/atlas-datascience/lion/-/epics/8; references=; created_at=2025-06-30T15:59:04.840Z; updated_at=2025-06-30T15:59:04.840Z; closed_at=; labels=System.Object[]; upvotes=0; downvotes=0; _links=}.due_date) { @{id=3674247; iid=8; color=#1068bf; text_color=#FFFFFF; group_id=101972113; parent_id=; parent_iid=; imported=False; imported_from=none; title=Cross-Account STS Role Framework for the Access Broker; description=
To honor the platform's "data-in-place" promise and its Least-Privilege & Short-Lived Credentials principle, we need a reusable, Terraform-first framework that lets the Access Broker assume tightly-scoped roles in any customer or staging account, using AWS STS with an ExternalId guard. The epic will deliver IaC module plus reference policies, CI checks, and integration tests so that the Access Broker can fetch a pre-signed S3 URL or RDS IAM token across account. It must support multiple trust patterns (single-tenant prod, sandbox, and shared-services accounts), emit CloudTrail and Athena-queryable audit records for every AssumeRole.


**Acceptance Criteria**

- Terraform module published in infra-iac/modules/sts-cross-account with variables for tenant_id, allowed_actions, external_id, max_session_seconds.
- AssumeRole trust policy enforces sts:ExternalId and restricts the principal to our Access Broker execution role ARN list.
- Managed-policy generator outputs the minimal IAM JSON based on a YAML allow-list of data-stores the tenant actually uses (e.g., s3:GetObject for specific buckets, rds-db:connect on a named cluster).
- CI pipeline runs tflint, terrascan, and checkov and fails if any policy grants broader than requested, or if max_session_seconds exceeds 900.
- End-to-end integration test spins up a throw-away account in AWS Control Tower, deploys the module, then exercises the Broker's /v1/issue endpoint to retrieve a presigned S3 URL and verifies object download succeeds while an out-of-scope bucket is denied.
- CloudTrail + Athena dashboards show every AssumeRole with sourceIdentity=access-broker and surface decision_id for audit.; confidential=False; author=; start_date=; start_date_is_fixed=False; start_date_fixed=; start_date_from_inherited_source=; start_date_from_milestones=; end_date=; due_date=; due_date_is_fixed=False; due_date_fixed=; due_date_from_inherited_source=; due_date_from_milestones=; state=opened; web_edit_url=/groups/atlas-datascience/lion/-/epics/8; web_url=https://gitlab.com/groups/atlas-datascience/lion/-/epics/8; references=; created_at=2025-06-30T15:59:04.840Z; updated_at=2025-06-30T15:59:04.840Z; closed_at=; labels=System.Object[]; upvotes=0; downvotes=0; _links=}.due_date } else { 'N/A' }

--- 
## Epic #7: Establish CICD Foundation
- **State**: opened
- **Start**: N/A
- **Due**: N/A
- **Created**: 2025-06-23T19:09:06.229Z
- **Web**: https://gitlab.com/groups/atlas-datascience/lion/-/epics/7

Build all the tooling within Gitlab to support the evaluation of software, container and infrastructure projects upon commit to their respective source repositories.  Use Gitlab best practices using job templates, catalogs for new code.  Ensure that logic is defined once and used everywhere.  Identify Gitlab templates for common jobs (i.e. SAST, DAST).

Container Pipelines Must:

* Build container
* Scan Image for vulnerabilities
* Create SBOM
* Run Container Structure Tests
* Save results into a body of evidence
* Publish results to Gitlab Vulnerability and Security Dashboard

Code Pipelines Must:

* Install dependencies
* Compile code
* Run Linting
* Run Unit Tests
* Run Code Scans (e.g. SAST, Dependency Checking, DAST)
* Package Code and push to Artifact Repository
* Publish results to Gitlab Vulnerability and Security Dashboard

Infrastructure Pipelines Must:

* Run Linting to check structure
* Run vulnerability checks
* Publish results to Gitlab Vulnerability and Security Dashboard

N/A = if (@{id=3664370; iid=7; color=#1068bf; text_color=#FFFFFF; group_id=101972113; parent_id=; parent_iid=; imported=False; imported_from=none; title=Establish CICD Foundation; description=Build all the tooling within Gitlab to support the evaluation of software, container and infrastructure projects upon commit to their respective source repositories.  Use Gitlab best practices using job templates, catalogs for new code.  Ensure that logic is defined once and used everywhere.  Identify Gitlab templates for common jobs (i.e. SAST, DAST).

Container Pipelines Must:

* Build container
* Scan Image for vulnerabilities
* Create SBOM
* Run Container Structure Tests
* Save results into a body of evidence
* Publish results to Gitlab Vulnerability and Security Dashboard

Code Pipelines Must:

* Install dependencies
* Compile code
* Run Linting
* Run Unit Tests
* Run Code Scans (e.g. SAST, Dependency Checking, DAST)
* Package Code and push to Artifact Repository
* Publish results to Gitlab Vulnerability and Security Dashboard

Infrastructure Pipelines Must:

* Run Linting to check structure
* Run vulnerability checks
* Publish results to Gitlab Vulnerability and Security Dashboard; confidential=False; author=; start_date=; start_date_is_fixed=False; start_date_fixed=; start_date_from_inherited_source=; start_date_from_milestones=; end_date=; due_date=; due_date_is_fixed=False; due_date_fixed=; due_date_from_inherited_source=; due_date_from_milestones=; state=opened; web_edit_url=/groups/atlas-datascience/lion/-/epics/7; web_url=https://gitlab.com/groups/atlas-datascience/lion/-/epics/7; references=; created_at=2025-06-23T19:09:06.229Z; updated_at=2025-06-23T19:38:19.162Z; closed_at=; labels=System.Object[]; upvotes=0; downvotes=0; _links=}.start_date) { @{id=3664370; iid=7; color=#1068bf; text_color=#FFFFFF; group_id=101972113; parent_id=; parent_iid=; imported=False; imported_from=none; title=Establish CICD Foundation; description=Build all the tooling within Gitlab to support the evaluation of software, container and infrastructure projects upon commit to their respective source repositories.  Use Gitlab best practices using job templates, catalogs for new code.  Ensure that logic is defined once and used everywhere.  Identify Gitlab templates for common jobs (i.e. SAST, DAST).

Container Pipelines Must:

* Build container
* Scan Image for vulnerabilities
* Create SBOM
* Run Container Structure Tests
* Save results into a body of evidence
* Publish results to Gitlab Vulnerability and Security Dashboard

Code Pipelines Must:

* Install dependencies
* Compile code
* Run Linting
* Run Unit Tests
* Run Code Scans (e.g. SAST, Dependency Checking, DAST)
* Package Code and push to Artifact Repository
* Publish results to Gitlab Vulnerability and Security Dashboard

Infrastructure Pipelines Must:

* Run Linting to check structure
* Run vulnerability checks
* Publish results to Gitlab Vulnerability and Security Dashboard; confidential=False; author=; start_date=; start_date_is_fixed=False; start_date_fixed=; start_date_from_inherited_source=; start_date_from_milestones=; end_date=; due_date=; due_date_is_fixed=False; due_date_fixed=; due_date_from_inherited_source=; due_date_from_milestones=; state=opened; web_edit_url=/groups/atlas-datascience/lion/-/epics/7; web_url=https://gitlab.com/groups/atlas-datascience/lion/-/epics/7; references=; created_at=2025-06-23T19:09:06.229Z; updated_at=2025-06-23T19:38:19.162Z; closed_at=; labels=System.Object[]; upvotes=0; downvotes=0; _links=}.start_date } else { 'N/A' }
N/A = if (@{id=3664370; iid=7; color=#1068bf; text_color=#FFFFFF; group_id=101972113; parent_id=; parent_iid=; imported=False; imported_from=none; title=Establish CICD Foundation; description=Build all the tooling within Gitlab to support the evaluation of software, container and infrastructure projects upon commit to their respective source repositories.  Use Gitlab best practices using job templates, catalogs for new code.  Ensure that logic is defined once and used everywhere.  Identify Gitlab templates for common jobs (i.e. SAST, DAST).

Container Pipelines Must:

* Build container
* Scan Image for vulnerabilities
* Create SBOM
* Run Container Structure Tests
* Save results into a body of evidence
* Publish results to Gitlab Vulnerability and Security Dashboard

Code Pipelines Must:

* Install dependencies
* Compile code
* Run Linting
* Run Unit Tests
* Run Code Scans (e.g. SAST, Dependency Checking, DAST)
* Package Code and push to Artifact Repository
* Publish results to Gitlab Vulnerability and Security Dashboard

Infrastructure Pipelines Must:

* Run Linting to check structure
* Run vulnerability checks
* Publish results to Gitlab Vulnerability and Security Dashboard; confidential=False; author=; start_date=; start_date_is_fixed=False; start_date_fixed=; start_date_from_inherited_source=; start_date_from_milestones=; end_date=; due_date=; due_date_is_fixed=False; due_date_fixed=; due_date_from_inherited_source=; due_date_from_milestones=; state=opened; web_edit_url=/groups/atlas-datascience/lion/-/epics/7; web_url=https://gitlab.com/groups/atlas-datascience/lion/-/epics/7; references=; created_at=2025-06-23T19:09:06.229Z; updated_at=2025-06-23T19:38:19.162Z; closed_at=; labels=System.Object[]; upvotes=0; downvotes=0; _links=}.due_date) { @{id=3664370; iid=7; color=#1068bf; text_color=#FFFFFF; group_id=101972113; parent_id=; parent_iid=; imported=False; imported_from=none; title=Establish CICD Foundation; description=Build all the tooling within Gitlab to support the evaluation of software, container and infrastructure projects upon commit to their respective source repositories.  Use Gitlab best practices using job templates, catalogs for new code.  Ensure that logic is defined once and used everywhere.  Identify Gitlab templates for common jobs (i.e. SAST, DAST).

Container Pipelines Must:

* Build container
* Scan Image for vulnerabilities
* Create SBOM
* Run Container Structure Tests
* Save results into a body of evidence
* Publish results to Gitlab Vulnerability and Security Dashboard

Code Pipelines Must:

* Install dependencies
* Compile code
* Run Linting
* Run Unit Tests
* Run Code Scans (e.g. SAST, Dependency Checking, DAST)
* Package Code and push to Artifact Repository
* Publish results to Gitlab Vulnerability and Security Dashboard

Infrastructure Pipelines Must:

* Run Linting to check structure
* Run vulnerability checks
* Publish results to Gitlab Vulnerability and Security Dashboard; confidential=False; author=; start_date=; start_date_is_fixed=False; start_date_fixed=; start_date_from_inherited_source=; start_date_from_milestones=; end_date=; due_date=; due_date_is_fixed=False; due_date_fixed=; due_date_from_inherited_source=; due_date_from_milestones=; state=opened; web_edit_url=/groups/atlas-datascience/lion/-/epics/7; web_url=https://gitlab.com/groups/atlas-datascience/lion/-/epics/7; references=; created_at=2025-06-23T19:09:06.229Z; updated_at=2025-06-23T19:38:19.162Z; closed_at=; labels=System.Object[]; upvotes=0; downvotes=0; _links=}.due_date } else { 'N/A' }

--- 
## Epic #6: Edge Connector v1
- **State**: opened
- **Start**: N/A
- **Due**: N/A
- **Created**: 2025-06-17T13:38:00.798Z
- **Web**: https://gitlab.com/groups/atlas-datascience/lion/-/epics/6

- Create a Lambda that listens to S3 `s3:ObjectCreated:*` /`s3:ObjectRemoved:*`/`s3:ObjectRestore:*`; 
- Create event JSON and posts encrypted object to the Ingestion Gateway ([Legacy](https://gitlab.com/ftsg/lion/core/data-in/extract-transform-load/inventory-enhancement/inventory-listener/-/blob/main/src/app.service.ts?ref_type=heads));
- Have local buffering in case of Ingestion Gateway issues;
- Implement metrics (JSON Logs, counts, failures, etc.);
- CI/CD automation.


Inventory Router from Legacy: [Link](https://teams.microsoft.com/l/message/19:meeting_YWM1YWFhMGUtOGI2Yy00MzQyLTgwNzctMTQ0MDJlM2MwN2I0@thread.v2/1750185285891?context=%7B%22contextType%22%3A%22chat%22%7D)
General Metadata Extraction: [Link](https://gitlab.com/ftsg/lion/core/data-in/extract-transform-load/inventory-enhancement/extraction/document-extraction/-/tree/main/src/doc_extractor?ref_type=heads)

**Events Flow:**
S3 Event -> SQS -> Lambda -> Moch Ingestion Gateway API


**Edge Connector CloudFormation/Terraform deploy template similar to [DataDog](https://docs.datadoghq.com/integrations/guide/):**
- Lambda Function connected to S3 events
- FIFO SQS for buffering
- IAM Role and Policy with defined access
- Secret Manager credentials for JWT
- CloudWatch Log Group for logs
- Would be nice to have basic alerting and metrics on top of cloudwatch

**Lambda Configurations:**
- GATEWAY_URL - Ingestion Gateway HTTPS endpoint
- TENANT_ID - Tenant scoping + idempotency key salt
- JWT_SECRET_ARN -  Secrets Manager ARN for JWT
- MAX_RETRY_ATTEMPTS - max retries before DQL
- INCLUDE_PREFIXES - S3 include prefixes, comma separated
- EXCLUDE_PREFIXES - S3 exclude prefixes, comma separated
- ENABLE_GZIP - enable compression
- LOG_LEVEL - Log level for debugging

N/A = if (@{id=3655658; iid=6; color=#1068bf; text_color=#FFFFFF; group_id=101972113; parent_id=; parent_iid=; imported=False; imported_from=none; title=Edge Connector v1; description=- Create a Lambda that listens to S3 `s3:ObjectCreated:*` /`s3:ObjectRemoved:*`/`s3:ObjectRestore:*`; 
- Create event JSON and posts encrypted object to the Ingestion Gateway ([Legacy](https://gitlab.com/ftsg/lion/core/data-in/extract-transform-load/inventory-enhancement/inventory-listener/-/blob/main/src/app.service.ts?ref_type=heads));
- Have local buffering in case of Ingestion Gateway issues;
- Implement metrics (JSON Logs, counts, failures, etc.);
- CI/CD automation.


Inventory Router from Legacy: [Link](https://teams.microsoft.com/l/message/19:meeting_YWM1YWFhMGUtOGI2Yy00MzQyLTgwNzctMTQ0MDJlM2MwN2I0@thread.v2/1750185285891?context=%7B%22contextType%22%3A%22chat%22%7D)
General Metadata Extraction: [Link](https://gitlab.com/ftsg/lion/core/data-in/extract-transform-load/inventory-enhancement/extraction/document-extraction/-/tree/main/src/doc_extractor?ref_type=heads)

**Events Flow:**
S3 Event -> SQS -> Lambda -> Moch Ingestion Gateway API


**Edge Connector CloudFormation/Terraform deploy template similar to [DataDog](https://docs.datadoghq.com/integrations/guide/):**
- Lambda Function connected to S3 events
- FIFO SQS for buffering
- IAM Role and Policy with defined access
- Secret Manager credentials for JWT
- CloudWatch Log Group for logs
- Would be nice to have basic alerting and metrics on top of cloudwatch

**Lambda Configurations:**
- GATEWAY_URL - Ingestion Gateway HTTPS endpoint
- TENANT_ID - Tenant scoping + idempotency key salt
- JWT_SECRET_ARN -  Secrets Manager ARN for JWT
- MAX_RETRY_ATTEMPTS - max retries before DQL
- INCLUDE_PREFIXES - S3 include prefixes, comma separated
- EXCLUDE_PREFIXES - S3 exclude prefixes, comma separated
- ENABLE_GZIP - enable compression
- LOG_LEVEL - Log level for debugging; confidential=False; author=; start_date=; start_date_is_fixed=False; start_date_fixed=; start_date_from_inherited_source=; start_date_from_milestones=; end_date=; due_date=; due_date_is_fixed=False; due_date_fixed=; due_date_from_inherited_source=; due_date_from_milestones=; state=opened; web_edit_url=/groups/atlas-datascience/lion/-/epics/6; web_url=https://gitlab.com/groups/atlas-datascience/lion/-/epics/6; references=; created_at=2025-06-17T13:38:00.798Z; updated_at=2025-06-23T19:38:30.779Z; closed_at=; labels=System.Object[]; upvotes=0; downvotes=0; _links=}.start_date) { @{id=3655658; iid=6; color=#1068bf; text_color=#FFFFFF; group_id=101972113; parent_id=; parent_iid=; imported=False; imported_from=none; title=Edge Connector v1; description=- Create a Lambda that listens to S3 `s3:ObjectCreated:*` /`s3:ObjectRemoved:*`/`s3:ObjectRestore:*`; 
- Create event JSON and posts encrypted object to the Ingestion Gateway ([Legacy](https://gitlab.com/ftsg/lion/core/data-in/extract-transform-load/inventory-enhancement/inventory-listener/-/blob/main/src/app.service.ts?ref_type=heads));
- Have local buffering in case of Ingestion Gateway issues;
- Implement metrics (JSON Logs, counts, failures, etc.);
- CI/CD automation.


Inventory Router from Legacy: [Link](https://teams.microsoft.com/l/message/19:meeting_YWM1YWFhMGUtOGI2Yy00MzQyLTgwNzctMTQ0MDJlM2MwN2I0@thread.v2/1750185285891?context=%7B%22contextType%22%3A%22chat%22%7D)
General Metadata Extraction: [Link](https://gitlab.com/ftsg/lion/core/data-in/extract-transform-load/inventory-enhancement/extraction/document-extraction/-/tree/main/src/doc_extractor?ref_type=heads)

**Events Flow:**
S3 Event -> SQS -> Lambda -> Moch Ingestion Gateway API


**Edge Connector CloudFormation/Terraform deploy template similar to [DataDog](https://docs.datadoghq.com/integrations/guide/):**
- Lambda Function connected to S3 events
- FIFO SQS for buffering
- IAM Role and Policy with defined access
- Secret Manager credentials for JWT
- CloudWatch Log Group for logs
- Would be nice to have basic alerting and metrics on top of cloudwatch

**Lambda Configurations:**
- GATEWAY_URL - Ingestion Gateway HTTPS endpoint
- TENANT_ID - Tenant scoping + idempotency key salt
- JWT_SECRET_ARN -  Secrets Manager ARN for JWT
- MAX_RETRY_ATTEMPTS - max retries before DQL
- INCLUDE_PREFIXES - S3 include prefixes, comma separated
- EXCLUDE_PREFIXES - S3 exclude prefixes, comma separated
- ENABLE_GZIP - enable compression
- LOG_LEVEL - Log level for debugging; confidential=False; author=; start_date=; start_date_is_fixed=False; start_date_fixed=; start_date_from_inherited_source=; start_date_from_milestones=; end_date=; due_date=; due_date_is_fixed=False; due_date_fixed=; due_date_from_inherited_source=; due_date_from_milestones=; state=opened; web_edit_url=/groups/atlas-datascience/lion/-/epics/6; web_url=https://gitlab.com/groups/atlas-datascience/lion/-/epics/6; references=; created_at=2025-06-17T13:38:00.798Z; updated_at=2025-06-23T19:38:30.779Z; closed_at=; labels=System.Object[]; upvotes=0; downvotes=0; _links=}.start_date } else { 'N/A' }
N/A = if (@{id=3655658; iid=6; color=#1068bf; text_color=#FFFFFF; group_id=101972113; parent_id=; parent_iid=; imported=False; imported_from=none; title=Edge Connector v1; description=- Create a Lambda that listens to S3 `s3:ObjectCreated:*` /`s3:ObjectRemoved:*`/`s3:ObjectRestore:*`; 
- Create event JSON and posts encrypted object to the Ingestion Gateway ([Legacy](https://gitlab.com/ftsg/lion/core/data-in/extract-transform-load/inventory-enhancement/inventory-listener/-/blob/main/src/app.service.ts?ref_type=heads));
- Have local buffering in case of Ingestion Gateway issues;
- Implement metrics (JSON Logs, counts, failures, etc.);
- CI/CD automation.


Inventory Router from Legacy: [Link](https://teams.microsoft.com/l/message/19:meeting_YWM1YWFhMGUtOGI2Yy00MzQyLTgwNzctMTQ0MDJlM2MwN2I0@thread.v2/1750185285891?context=%7B%22contextType%22%3A%22chat%22%7D)
General Metadata Extraction: [Link](https://gitlab.com/ftsg/lion/core/data-in/extract-transform-load/inventory-enhancement/extraction/document-extraction/-/tree/main/src/doc_extractor?ref_type=heads)

**Events Flow:**
S3 Event -> SQS -> Lambda -> Moch Ingestion Gateway API


**Edge Connector CloudFormation/Terraform deploy template similar to [DataDog](https://docs.datadoghq.com/integrations/guide/):**
- Lambda Function connected to S3 events
- FIFO SQS for buffering
- IAM Role and Policy with defined access
- Secret Manager credentials for JWT
- CloudWatch Log Group for logs
- Would be nice to have basic alerting and metrics on top of cloudwatch

**Lambda Configurations:**
- GATEWAY_URL - Ingestion Gateway HTTPS endpoint
- TENANT_ID - Tenant scoping + idempotency key salt
- JWT_SECRET_ARN -  Secrets Manager ARN for JWT
- MAX_RETRY_ATTEMPTS - max retries before DQL
- INCLUDE_PREFIXES - S3 include prefixes, comma separated
- EXCLUDE_PREFIXES - S3 exclude prefixes, comma separated
- ENABLE_GZIP - enable compression
- LOG_LEVEL - Log level for debugging; confidential=False; author=; start_date=; start_date_is_fixed=False; start_date_fixed=; start_date_from_inherited_source=; start_date_from_milestones=; end_date=; due_date=; due_date_is_fixed=False; due_date_fixed=; due_date_from_inherited_source=; due_date_from_milestones=; state=opened; web_edit_url=/groups/atlas-datascience/lion/-/epics/6; web_url=https://gitlab.com/groups/atlas-datascience/lion/-/epics/6; references=; created_at=2025-06-17T13:38:00.798Z; updated_at=2025-06-23T19:38:30.779Z; closed_at=; labels=System.Object[]; upvotes=0; downvotes=0; _links=}.due_date) { @{id=3655658; iid=6; color=#1068bf; text_color=#FFFFFF; group_id=101972113; parent_id=; parent_iid=; imported=False; imported_from=none; title=Edge Connector v1; description=- Create a Lambda that listens to S3 `s3:ObjectCreated:*` /`s3:ObjectRemoved:*`/`s3:ObjectRestore:*`; 
- Create event JSON and posts encrypted object to the Ingestion Gateway ([Legacy](https://gitlab.com/ftsg/lion/core/data-in/extract-transform-load/inventory-enhancement/inventory-listener/-/blob/main/src/app.service.ts?ref_type=heads));
- Have local buffering in case of Ingestion Gateway issues;
- Implement metrics (JSON Logs, counts, failures, etc.);
- CI/CD automation.


Inventory Router from Legacy: [Link](https://teams.microsoft.com/l/message/19:meeting_YWM1YWFhMGUtOGI2Yy00MzQyLTgwNzctMTQ0MDJlM2MwN2I0@thread.v2/1750185285891?context=%7B%22contextType%22%3A%22chat%22%7D)
General Metadata Extraction: [Link](https://gitlab.com/ftsg/lion/core/data-in/extract-transform-load/inventory-enhancement/extraction/document-extraction/-/tree/main/src/doc_extractor?ref_type=heads)

**Events Flow:**
S3 Event -> SQS -> Lambda -> Moch Ingestion Gateway API


**Edge Connector CloudFormation/Terraform deploy template similar to [DataDog](https://docs.datadoghq.com/integrations/guide/):**
- Lambda Function connected to S3 events
- FIFO SQS for buffering
- IAM Role and Policy with defined access
- Secret Manager credentials for JWT
- CloudWatch Log Group for logs
- Would be nice to have basic alerting and metrics on top of cloudwatch

**Lambda Configurations:**
- GATEWAY_URL - Ingestion Gateway HTTPS endpoint
- TENANT_ID - Tenant scoping + idempotency key salt
- JWT_SECRET_ARN -  Secrets Manager ARN for JWT
- MAX_RETRY_ATTEMPTS - max retries before DQL
- INCLUDE_PREFIXES - S3 include prefixes, comma separated
- EXCLUDE_PREFIXES - S3 exclude prefixes, comma separated
- ENABLE_GZIP - enable compression
- LOG_LEVEL - Log level for debugging; confidential=False; author=; start_date=; start_date_is_fixed=False; start_date_fixed=; start_date_from_inherited_source=; start_date_from_milestones=; end_date=; due_date=; due_date_is_fixed=False; due_date_fixed=; due_date_from_inherited_source=; due_date_from_milestones=; state=opened; web_edit_url=/groups/atlas-datascience/lion/-/epics/6; web_url=https://gitlab.com/groups/atlas-datascience/lion/-/epics/6; references=; created_at=2025-06-17T13:38:00.798Z; updated_at=2025-06-23T19:38:30.779Z; closed_at=; labels=System.Object[]; upvotes=0; downvotes=0; _links=}.due_date } else { 'N/A' }

--- 
## Epic #5: Local Development Stack
- **State**: opened
- **Start**: N/A
- **Due**: N/A
- **Created**: 2025-06-09T13:47:32.644Z
- **Web**: https://gitlab.com/groups/atlas-datascience/lion/-/epics/5

Deliver dockerâcompose.yml, Makefile and bootstrap scripts so engineers can spin local dev stack.



Example File Locations in https://gitlab.com/ftsg/lion/example-datasets repository:
- GeoTIFF samples: example-datasets-geo-tif
- GeoPDF samples: example-datasets-geo-pdf
- Standard TIFF samples: example-datasets-tif

Success Metrics:
- All three connector types successfully ingest sample data
- Spatial metadata is correctly stored in PostGIS
- OpenMetadata catalog displays geospatial information
- Local development environment supports easy testing and development

N/A = if (@{id=3642066; iid=5; color=#1068bf; text_color=#FFFFFF; group_id=101972113; parent_id=; parent_iid=; imported=False; imported_from=none; title=Local Development Stack; description=Deliver dockerâcompose.yml, Makefile and bootstrap scripts so engineers can spin local dev stack.



Example File Locations in https://gitlab.com/ftsg/lion/example-datasets repository:
- GeoTIFF samples: example-datasets-geo-tif
- GeoPDF samples: example-datasets-geo-pdf
- Standard TIFF samples: example-datasets-tif

Success Metrics:
- All three connector types successfully ingest sample data
- Spatial metadata is correctly stored in PostGIS
- OpenMetadata catalog displays geospatial information
- Local development environment supports easy testing and development; confidential=False; author=; start_date=; start_date_is_fixed=False; start_date_fixed=; start_date_from_inherited_source=; start_date_from_milestones=; end_date=; due_date=; due_date_is_fixed=False; due_date_fixed=; due_date_from_inherited_source=; due_date_from_milestones=; state=opened; web_edit_url=/groups/atlas-datascience/lion/-/epics/5; web_url=https://gitlab.com/groups/atlas-datascience/lion/-/epics/5; references=; created_at=2025-06-09T13:47:32.644Z; updated_at=2025-06-09T15:57:45.578Z; closed_at=; labels=System.Object[]; upvotes=0; downvotes=0; _links=}.start_date) { @{id=3642066; iid=5; color=#1068bf; text_color=#FFFFFF; group_id=101972113; parent_id=; parent_iid=; imported=False; imported_from=none; title=Local Development Stack; description=Deliver dockerâcompose.yml, Makefile and bootstrap scripts so engineers can spin local dev stack.



Example File Locations in https://gitlab.com/ftsg/lion/example-datasets repository:
- GeoTIFF samples: example-datasets-geo-tif
- GeoPDF samples: example-datasets-geo-pdf
- Standard TIFF samples: example-datasets-tif

Success Metrics:
- All three connector types successfully ingest sample data
- Spatial metadata is correctly stored in PostGIS
- OpenMetadata catalog displays geospatial information
- Local development environment supports easy testing and development; confidential=False; author=; start_date=; start_date_is_fixed=False; start_date_fixed=; start_date_from_inherited_source=; start_date_from_milestones=; end_date=; due_date=; due_date_is_fixed=False; due_date_fixed=; due_date_from_inherited_source=; due_date_from_milestones=; state=opened; web_edit_url=/groups/atlas-datascience/lion/-/epics/5; web_url=https://gitlab.com/groups/atlas-datascience/lion/-/epics/5; references=; created_at=2025-06-09T13:47:32.644Z; updated_at=2025-06-09T15:57:45.578Z; closed_at=; labels=System.Object[]; upvotes=0; downvotes=0; _links=}.start_date } else { 'N/A' }
N/A = if (@{id=3642066; iid=5; color=#1068bf; text_color=#FFFFFF; group_id=101972113; parent_id=; parent_iid=; imported=False; imported_from=none; title=Local Development Stack; description=Deliver dockerâcompose.yml, Makefile and bootstrap scripts so engineers can spin local dev stack.



Example File Locations in https://gitlab.com/ftsg/lion/example-datasets repository:
- GeoTIFF samples: example-datasets-geo-tif
- GeoPDF samples: example-datasets-geo-pdf
- Standard TIFF samples: example-datasets-tif

Success Metrics:
- All three connector types successfully ingest sample data
- Spatial metadata is correctly stored in PostGIS
- OpenMetadata catalog displays geospatial information
- Local development environment supports easy testing and development; confidential=False; author=; start_date=; start_date_is_fixed=False; start_date_fixed=; start_date_from_inherited_source=; start_date_from_milestones=; end_date=; due_date=; due_date_is_fixed=False; due_date_fixed=; due_date_from_inherited_source=; due_date_from_milestones=; state=opened; web_edit_url=/groups/atlas-datascience/lion/-/epics/5; web_url=https://gitlab.com/groups/atlas-datascience/lion/-/epics/5; references=; created_at=2025-06-09T13:47:32.644Z; updated_at=2025-06-09T15:57:45.578Z; closed_at=; labels=System.Object[]; upvotes=0; downvotes=0; _links=}.due_date) { @{id=3642066; iid=5; color=#1068bf; text_color=#FFFFFF; group_id=101972113; parent_id=; parent_iid=; imported=False; imported_from=none; title=Local Development Stack; description=Deliver dockerâcompose.yml, Makefile and bootstrap scripts so engineers can spin local dev stack.



Example File Locations in https://gitlab.com/ftsg/lion/example-datasets repository:
- GeoTIFF samples: example-datasets-geo-tif
- GeoPDF samples: example-datasets-geo-pdf
- Standard TIFF samples: example-datasets-tif

Success Metrics:
- All three connector types successfully ingest sample data
- Spatial metadata is correctly stored in PostGIS
- OpenMetadata catalog displays geospatial information
- Local development environment supports easy testing and development; confidential=False; author=; start_date=; start_date_is_fixed=False; start_date_fixed=; start_date_from_inherited_source=; start_date_from_milestones=; end_date=; due_date=; due_date_is_fixed=False; due_date_fixed=; due_date_from_inherited_source=; due_date_from_milestones=; state=opened; web_edit_url=/groups/atlas-datascience/lion/-/epics/5; web_url=https://gitlab.com/groups/atlas-datascience/lion/-/epics/5; references=; created_at=2025-06-09T13:47:32.644Z; updated_at=2025-06-09T15:57:45.578Z; closed_at=; labels=System.Object[]; upvotes=0; downvotes=0; _links=}.due_date } else { 'N/A' }

--- 
## Epic #3: Dissemination and Release
- **State**: opened
- **Start**: N/A
- **Due**: N/A
- **Created**: 2025-03-17T15:10:39.627Z
- **Web**: https://gitlab.com/groups/atlas-datascience/lion/-/epics/3

Integrate dissemination components with catalog (Adjust)

* Dissemination components utilize catalog to verify the requestor can have the object or objects created.  Given a list of record identifiers the dissemination components look up those items in the catalog utilizing the requestors' claims.  Objects that cannot be delivered will result in a 404 from the catalog if the requestor cannot see the existence of the object, a response with no path to the object in the case of fact-of, and the full record in the case of a disseminateable object.
* In the case of a single object the response can be streamed immediately.  In the case of a collection of objects an appropriate response is to be provided so that the requestor can check on the status of the request and when bundling is complete come back and stream the bundle of requested objects.
* Collect telemetry data on the number of requests, by whom, for what and the result

N/A = if (@{id=3530844; iid=3; color=#1068bf; text_color=#FFFFFF; group_id=101972113; parent_id=; parent_iid=; imported=False; imported_from=none; title=Dissemination and Release; description=Integrate dissemination components with catalog (Adjust)

* Dissemination components utilize catalog to verify the requestor can have the object or objects created.  Given a list of record identifiers the dissemination components look up those items in the catalog utilizing the requestors' claims.  Objects that cannot be delivered will result in a 404 from the catalog if the requestor cannot see the existence of the object, a response with no path to the object in the case of fact-of, and the full record in the case of a disseminateable object.
* In the case of a single object the response can be streamed immediately.  In the case of a collection of objects an appropriate response is to be provided so that the requestor can check on the status of the request and when bundling is complete come back and stream the bundle of requested objects.
* Collect telemetry data on the number of requests, by whom, for what and the result; confidential=True; author=; start_date=; start_date_is_fixed=False; start_date_fixed=; start_date_from_inherited_source=; start_date_from_milestones=; end_date=; due_date=; due_date_is_fixed=False; due_date_fixed=; due_date_from_inherited_source=; due_date_from_milestones=; state=opened; web_edit_url=/groups/atlas-datascience/lion/-/epics/3; web_url=https://gitlab.com/groups/atlas-datascience/lion/-/epics/3; references=; created_at=2025-03-17T15:10:39.627Z; updated_at=2025-03-17T15:19:58.621Z; closed_at=; labels=System.Object[]; upvotes=0; downvotes=0; _links=}.start_date) { @{id=3530844; iid=3; color=#1068bf; text_color=#FFFFFF; group_id=101972113; parent_id=; parent_iid=; imported=False; imported_from=none; title=Dissemination and Release; description=Integrate dissemination components with catalog (Adjust)

* Dissemination components utilize catalog to verify the requestor can have the object or objects created.  Given a list of record identifiers the dissemination components look up those items in the catalog utilizing the requestors' claims.  Objects that cannot be delivered will result in a 404 from the catalog if the requestor cannot see the existence of the object, a response with no path to the object in the case of fact-of, and the full record in the case of a disseminateable object.
* In the case of a single object the response can be streamed immediately.  In the case of a collection of objects an appropriate response is to be provided so that the requestor can check on the status of the request and when bundling is complete come back and stream the bundle of requested objects.
* Collect telemetry data on the number of requests, by whom, for what and the result; confidential=True; author=; start_date=; start_date_is_fixed=False; start_date_fixed=; start_date_from_inherited_source=; start_date_from_milestones=; end_date=; due_date=; due_date_is_fixed=False; due_date_fixed=; due_date_from_inherited_source=; due_date_from_milestones=; state=opened; web_edit_url=/groups/atlas-datascience/lion/-/epics/3; web_url=https://gitlab.com/groups/atlas-datascience/lion/-/epics/3; references=; created_at=2025-03-17T15:10:39.627Z; updated_at=2025-03-17T15:19:58.621Z; closed_at=; labels=System.Object[]; upvotes=0; downvotes=0; _links=}.start_date } else { 'N/A' }
N/A = if (@{id=3530844; iid=3; color=#1068bf; text_color=#FFFFFF; group_id=101972113; parent_id=; parent_iid=; imported=False; imported_from=none; title=Dissemination and Release; description=Integrate dissemination components with catalog (Adjust)

* Dissemination components utilize catalog to verify the requestor can have the object or objects created.  Given a list of record identifiers the dissemination components look up those items in the catalog utilizing the requestors' claims.  Objects that cannot be delivered will result in a 404 from the catalog if the requestor cannot see the existence of the object, a response with no path to the object in the case of fact-of, and the full record in the case of a disseminateable object.
* In the case of a single object the response can be streamed immediately.  In the case of a collection of objects an appropriate response is to be provided so that the requestor can check on the status of the request and when bundling is complete come back and stream the bundle of requested objects.
* Collect telemetry data on the number of requests, by whom, for what and the result; confidential=True; author=; start_date=; start_date_is_fixed=False; start_date_fixed=; start_date_from_inherited_source=; start_date_from_milestones=; end_date=; due_date=; due_date_is_fixed=False; due_date_fixed=; due_date_from_inherited_source=; due_date_from_milestones=; state=opened; web_edit_url=/groups/atlas-datascience/lion/-/epics/3; web_url=https://gitlab.com/groups/atlas-datascience/lion/-/epics/3; references=; created_at=2025-03-17T15:10:39.627Z; updated_at=2025-03-17T15:19:58.621Z; closed_at=; labels=System.Object[]; upvotes=0; downvotes=0; _links=}.due_date) { @{id=3530844; iid=3; color=#1068bf; text_color=#FFFFFF; group_id=101972113; parent_id=; parent_iid=; imported=False; imported_from=none; title=Dissemination and Release; description=Integrate dissemination components with catalog (Adjust)

* Dissemination components utilize catalog to verify the requestor can have the object or objects created.  Given a list of record identifiers the dissemination components look up those items in the catalog utilizing the requestors' claims.  Objects that cannot be delivered will result in a 404 from the catalog if the requestor cannot see the existence of the object, a response with no path to the object in the case of fact-of, and the full record in the case of a disseminateable object.
* In the case of a single object the response can be streamed immediately.  In the case of a collection of objects an appropriate response is to be provided so that the requestor can check on the status of the request and when bundling is complete come back and stream the bundle of requested objects.
* Collect telemetry data on the number of requests, by whom, for what and the result; confidential=True; author=; start_date=; start_date_is_fixed=False; start_date_fixed=; start_date_from_inherited_source=; start_date_from_milestones=; end_date=; due_date=; due_date_is_fixed=False; due_date_fixed=; due_date_from_inherited_source=; due_date_from_milestones=; state=opened; web_edit_url=/groups/atlas-datascience/lion/-/epics/3; web_url=https://gitlab.com/groups/atlas-datascience/lion/-/epics/3; references=; created_at=2025-03-17T15:10:39.627Z; updated_at=2025-03-17T15:19:58.621Z; closed_at=; labels=System.Object[]; upvotes=0; downvotes=0; _links=}.due_date } else { 'N/A' }

--- 
## Epic #2: Inventory Creation and Maintenance
- **State**: opened
- **Start**: N/A
- **Due**: N/A
- **Created**: 2025-03-17T14:56:03.372Z
- **Web**: https://gitlab.com/groups/atlas-datascience/lion/-/epics/2

(Adjust) Migrate Data Ingest components to leveraging updated catalog interface to discover what content is present and building inventory records using that definition at time of ingest.

1. Inventory Processing
   * Dynamic discovery of data and metadata structures utilized by inventory, extraction and conditioning functions
   * Routing is determined by data definition as best as possible
   * Mapping to namespace is determined by the namespace definition defined in the catalog
   * Events are an indication of change is a trusted object store
2. (Update) Event Integration
   * Build an administrative interface for adding new Bucket events to listerner interface
   * Currently, there is a project for this that delivers the configuration change through CloudFormation Stack updates.  While this is configuration as code it may not be the most optimal way to deliver these kinds of adjustments.

N/A = if (@{id=3530833; iid=2; color=#1068bf; text_color=#FFFFFF; group_id=101972113; parent_id=; parent_iid=; imported=False; imported_from=none; title=Inventory Creation and Maintenance; description=(Adjust) Migrate Data Ingest components to leveraging updated catalog interface to discover what content is present and building inventory records using that definition at time of ingest.

1. Inventory Processing
   * Dynamic discovery of data and metadata structures utilized by inventory, extraction and conditioning functions
   * Routing is determined by data definition as best as possible
   * Mapping to namespace is determined by the namespace definition defined in the catalog
   * Events are an indication of change is a trusted object store
2. (Update) Event Integration
   * Build an administrative interface for adding new Bucket events to listerner interface
   * Currently, there is a project for this that delivers the configuration change through CloudFormation Stack updates.  While this is configuration as code it may not be the most optimal way to deliver these kinds of adjustments.; confidential=True; author=; start_date=; start_date_is_fixed=False; start_date_fixed=; start_date_from_inherited_source=; start_date_from_milestones=; end_date=; due_date=; due_date_is_fixed=False; due_date_fixed=; due_date_from_inherited_source=; due_date_from_milestones=; state=opened; web_edit_url=/groups/atlas-datascience/lion/-/epics/2; web_url=https://gitlab.com/groups/atlas-datascience/lion/-/epics/2; references=; created_at=2025-03-17T14:56:03.372Z; updated_at=2025-03-17T15:20:00.255Z; closed_at=; labels=System.Object[]; upvotes=0; downvotes=0; _links=}.start_date) { @{id=3530833; iid=2; color=#1068bf; text_color=#FFFFFF; group_id=101972113; parent_id=; parent_iid=; imported=False; imported_from=none; title=Inventory Creation and Maintenance; description=(Adjust) Migrate Data Ingest components to leveraging updated catalog interface to discover what content is present and building inventory records using that definition at time of ingest.

1. Inventory Processing
   * Dynamic discovery of data and metadata structures utilized by inventory, extraction and conditioning functions
   * Routing is determined by data definition as best as possible
   * Mapping to namespace is determined by the namespace definition defined in the catalog
   * Events are an indication of change is a trusted object store
2. (Update) Event Integration
   * Build an administrative interface for adding new Bucket events to listerner interface
   * Currently, there is a project for this that delivers the configuration change through CloudFormation Stack updates.  While this is configuration as code it may not be the most optimal way to deliver these kinds of adjustments.; confidential=True; author=; start_date=; start_date_is_fixed=False; start_date_fixed=; start_date_from_inherited_source=; start_date_from_milestones=; end_date=; due_date=; due_date_is_fixed=False; due_date_fixed=; due_date_from_inherited_source=; due_date_from_milestones=; state=opened; web_edit_url=/groups/atlas-datascience/lion/-/epics/2; web_url=https://gitlab.com/groups/atlas-datascience/lion/-/epics/2; references=; created_at=2025-03-17T14:56:03.372Z; updated_at=2025-03-17T15:20:00.255Z; closed_at=; labels=System.Object[]; upvotes=0; downvotes=0; _links=}.start_date } else { 'N/A' }
N/A = if (@{id=3530833; iid=2; color=#1068bf; text_color=#FFFFFF; group_id=101972113; parent_id=; parent_iid=; imported=False; imported_from=none; title=Inventory Creation and Maintenance; description=(Adjust) Migrate Data Ingest components to leveraging updated catalog interface to discover what content is present and building inventory records using that definition at time of ingest.

1. Inventory Processing
   * Dynamic discovery of data and metadata structures utilized by inventory, extraction and conditioning functions
   * Routing is determined by data definition as best as possible
   * Mapping to namespace is determined by the namespace definition defined in the catalog
   * Events are an indication of change is a trusted object store
2. (Update) Event Integration
   * Build an administrative interface for adding new Bucket events to listerner interface
   * Currently, there is a project for this that delivers the configuration change through CloudFormation Stack updates.  While this is configuration as code it may not be the most optimal way to deliver these kinds of adjustments.; confidential=True; author=; start_date=; start_date_is_fixed=False; start_date_fixed=; start_date_from_inherited_source=; start_date_from_milestones=; end_date=; due_date=; due_date_is_fixed=False; due_date_fixed=; due_date_from_inherited_source=; due_date_from_milestones=; state=opened; web_edit_url=/groups/atlas-datascience/lion/-/epics/2; web_url=https://gitlab.com/groups/atlas-datascience/lion/-/epics/2; references=; created_at=2025-03-17T14:56:03.372Z; updated_at=2025-03-17T15:20:00.255Z; closed_at=; labels=System.Object[]; upvotes=0; downvotes=0; _links=}.due_date) { @{id=3530833; iid=2; color=#1068bf; text_color=#FFFFFF; group_id=101972113; parent_id=; parent_iid=; imported=False; imported_from=none; title=Inventory Creation and Maintenance; description=(Adjust) Migrate Data Ingest components to leveraging updated catalog interface to discover what content is present and building inventory records using that definition at time of ingest.

1. Inventory Processing
   * Dynamic discovery of data and metadata structures utilized by inventory, extraction and conditioning functions
   * Routing is determined by data definition as best as possible
   * Mapping to namespace is determined by the namespace definition defined in the catalog
   * Events are an indication of change is a trusted object store
2. (Update) Event Integration
   * Build an administrative interface for adding new Bucket events to listerner interface
   * Currently, there is a project for this that delivers the configuration change through CloudFormation Stack updates.  While this is configuration as code it may not be the most optimal way to deliver these kinds of adjustments.; confidential=True; author=; start_date=; start_date_is_fixed=False; start_date_fixed=; start_date_from_inherited_source=; start_date_from_milestones=; end_date=; due_date=; due_date_is_fixed=False; due_date_fixed=; due_date_from_inherited_source=; due_date_from_milestones=; state=opened; web_edit_url=/groups/atlas-datascience/lion/-/epics/2; web_url=https://gitlab.com/groups/atlas-datascience/lion/-/epics/2; references=; created_at=2025-03-17T14:56:03.372Z; updated_at=2025-03-17T15:20:00.255Z; closed_at=; labels=System.Object[]; upvotes=0; downvotes=0; _links=}.due_date } else { 'N/A' }

--- 
